{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from transformers import BertTokenizer, BertForSequenceClassification\nfrom transformers import get_linear_schedule_with_warmup\nfrom torch.optim import AdamW\nimport torch\nfrom torch.utils.data import TensorDataset, DataLoader\nfrom torch.utils.data import RandomSampler, SequentialSampler\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import accuracy_score, confusion_matrix, f1_score, classification_report, precision_score, recall_score\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport time\nimport datetime\nimport random\nimport os\nimport helper_functions as hf"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1500\n",
      "1    500\n",
      "2    500\n",
      "0    500\n",
      "Name: label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_excel(\"../../fake_news_covid.xlsx\")\n",
    "data = data[[\"label_text\", \"text\"]]\n",
    "dataF = data[data.label_text==\"F\"][0:500]\n",
    "dataT = data[data.label_text==\"T\"][0:500]\n",
    "dataU = data[data.label_text==\"U\"][0:500]\n",
    "data = pd.concat([dataF, dataT, dataU])\n",
    "len(data)\n",
    "data[\"label_text\"].value_counts()\n",
    "data = data.sample(frac=1).reset_index(drop=True)\n",
    "print(len(data))\n",
    "data[\"text\"] = data[\"text\"].str.replace('\\n', ' ')\n",
    "data[\"label\"] = data[\"label_text\"].astype('category')\n",
    "data[\"label\"] = data[\"label\"].cat.codes\n",
    "print(data[\"label\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     1,500 comments\n",
      "   Min length: 7 tokens\n",
      "   Max length: 185 tokens\n",
      "Median length: 24.0 tokens\n",
      "2 of 1,500 sentences (0.1%) in the training set are longer than 160 tokens.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms).\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"bert-large-uncased\", do_lower_case=True)\n",
    "\n",
    "input_ids = []\n",
    "lengths = []\n",
    "for x, row in data.iterrows():\n",
    "    encoded_sent = tokenizer.encode(\n",
    "                        row['text'],                      \n",
    "                        add_special_tokens = True,\n",
    "                   )\n",
    "    input_ids.append(encoded_sent)\n",
    "    lengths.append(len(encoded_sent))\n",
    "\n",
    "print('{:>10,} comments'.format(len(input_ids)))\n",
    "print('   Min length: {:,} tokens'.format(min(lengths)))\n",
    "print('   Max length: {:,} tokens'.format(max(lengths)))\n",
    "print('Median length: {:,} tokens'.format(np.median(lengths)))\n",
    "\n",
    "hf.plot_distribution(lengths)\n",
    "\n",
    "max_len = 160 #max(lengths)\n",
    "\n",
    "num_truncated = np.sum(np.greater(lengths, max_len))\n",
    "num_sentences = len(lengths)\n",
    "prcnt = float(num_truncated) / float(num_sentences)\n",
    "print('{:,} of {:,} sentences ({:.1%}) in the training set are longer than {:} tokens.'.format(num_truncated, num_sentences, prcnt, max_len))\n",
    "\n",
    "# create tokenized data\n",
    "labels = []\n",
    "input_ids = []\n",
    "attn_masks = []\n",
    "\n",
    "for x, row in data.iterrows():\n",
    "    encoded_dict = tokenizer.encode_plus(row['text'],\n",
    "                                              max_length=max_len, #see other code for how to set this\n",
    "                                              padding='max_length',\n",
    "                                              truncation=True,\n",
    "                                              return_tensors='pt')\n",
    "    input_ids.append(encoded_dict['input_ids'])\n",
    "    attn_masks.append(encoded_dict['attention_mask'])\n",
    "    labels.append(row['label'])\n",
    "\n",
    "\n",
    "# Convert into tensor matrix.\n",
    "input_ids = torch.cat(input_ids, dim=0)\n",
    "attn_masks = torch.cat(attn_masks, dim=0)\n",
    "\n",
    "# Labels list to tensor.\n",
    "labels = torch.tensor(labels)\n",
    "\n",
    "# Create TensorDataset.\n",
    "dataset = TensorDataset(input_ids, attn_masks, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########\n",
    "# Specify key model parameters here: \n",
    "model_name = \"bert-large-uncased\"\n",
    "lr = 3e-5\n",
    "epochs = 5\n",
    "batch_size = 32\n",
    "#########"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_val = 6\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)\n",
    "torch.cuda.empty_cache() #Clear GPU cache if necessary\n",
    "\n",
    "training_stats = [] # Store training and validation loss,validation accuracy, and timings.\n",
    "fold_stats = []\n",
    "\n",
    "total_t0 = time.time() # Measure the total training time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ======================================== #\n#              CV Training                 #\n# ======================================== #\n  \nk_folds = 10\nkfold = KFold(n_splits=k_folds, shuffle=True)\n\ntimestamp = datetime.datetime.fromtimestamp(time.time()).strftime('%Y-%m-%d %H%M')\n\nfor fold, (train_ids, test_ids) in enumerate(kfold.split(dataset)):\n    \n    # Print\n    print(f'FOLD {fold+1}')\n    print('--------------------------------')\n    \n    # Sample elements randomly from a given list of ids, no replacement.\n    train_subsampler = torch.utils.data.SubsetRandomSampler(train_ids)\n    test_subsampler = torch.utils.data.SubsetRandomSampler(test_ids)\n    \n    # Define data loaders for training and testing data in this fold\n    train_dataloader = torch.utils.data.DataLoader(\n                      dataset, \n                      batch_size=batch_size, sampler=train_subsampler)\n    test_dataloader = torch.utils.data.DataLoader(\n                      dataset,\n                      batch_size=batch_size, sampler=test_subsampler)\n    \n    # Initiate model parameters for each fold\n    model = BertForSequenceClassification.from_pretrained(model_name, num_labels=3)\n    device = torch.device('cuda:0')\n    desc = model.to(device)\n    optimizer = AdamW(model.parameters(), lr = lr, eps = 1e-6) \n    total_steps = (int(len(dataset)/batch_size)+1) * epochs \n    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps = 10, num_training_steps = total_steps)\n          \n    # Run the training loop for defined number of epochs\n    for epoch_i in range(0, epochs):\n        print(\"\")\n        print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n        print('Training...')\n        t0 = time.time()\n        total_train_loss = 0 # Reset the total loss for this epoch.\n        model.train() # Put the model into training mode.\n        update_interval = hf.good_update_interval( # Pick an interval on which to print progress updates.\n                    total_iters = len(train_dataloader),\n                    num_desired_updates = 10\n                )\n\n        predictions_t, true_labels_t = [], []\n        for step, batch in enumerate(train_dataloader):\n            if (step % update_interval) == 0 and not step == 0:\n                elapsed = hf.format_time(time.time() - t0)\n                print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed), end='\\r')\n            b_input_ids = batch[0].to(device)\n            b_input_mask = batch[1].to(device)\n            b_labels = batch[2].to(device)\n            # Always clear any previously calculated gradients before performing a backward pass.\n            model.zero_grad()\n            # Perform a forward pass --returns the loss and the \"logits\"\n            outputs = model(b_input_ids,\n                               attention_mask=b_input_mask,\n                               labels=b_labels)\n            loss, logits = outputs.loss, outputs.logits\n\n            # Accumulate the training loss over all of the batches so that we can calculate the average loss at the end.\n            total_train_loss += loss.item()\n            # Perform a backward pass to calculate the gradients.\n            loss.backward()\n            # Clip the norm of the gradients to 1.0. This is to help prevent the \"exploding gradients\" problem.\n            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n            # Update parameters and take a step using the computed gradient.\n            optimizer.step()\n            # Update the learning rate.\n            scheduler.step()\n            \n            logits = logits.detach().cpu().numpy()\n            label_ids = b_labels.to('cpu').numpy()\n            # Store predictions and true labels\n            predictions_t.append(logits)\n            true_labels_t.append(label_ids)\n\n        # Combine the results across all batches.\n        flat_predictions_t = np.concatenate(predictions_t, axis=0)\n        flat_true_labels_t = np.concatenate(true_labels_t, axis=0)\n        # For each sample, pick the label (0, 1) with the highest score.\n        predicted_labels_t = np.argmax(flat_predictions_t, axis=1).flatten()        \n        acc_t = accuracy_score(predicted_labels_t, flat_true_labels_t)\n        \n        # Calculate the average loss over all of the batches.\n        avg_train_loss = total_train_loss / len(train_dataloader)\n\n        # Measure how long this epoch took.\n        training_time = hf.format_time(time.time() - t0)\n\n        print(\"\")\n        print(\"  Average training loss: {0:.3f}\".format(avg_train_loss))\n        print(\"  Training epoch took: {:}\".format(training_time))\n        print(\"  Training accuracy: {:.3f}\".format(acc_t))\n        \n        # Early stop based on training accuracy (heuristic to save compute, but validation loss is recommended)\n        if acc_t > 0.85 and epoch_i >= 2:\n            break        \n\n    # TEST\n    # After the completion of each training epoch, measure our performance on our test set.\n\n    print(\"\")\n    print(\"Running test...\")\n    t0 = time.time()\n    model.eval() # Put the model in evaluation mode--the dropout layers behave differently during evaluation.\n    total_eval_loss = 0\n    predictions, true_labels = [], []\n    # Evaluate data for one epoch\n    for batch in test_dataloader:\n        b_input_ids = batch[0].to(device)\n        b_input_mask = batch[1].to(device)\n        b_labels = batch[2].to(device)\n        with torch.no_grad():\n            # Forward pass, calculate logit predictions.\n            outputs = model(b_input_ids,\n                               attention_mask=b_input_mask,\n                               labels=b_labels)\n            loss, logits = outputs.loss, outputs.logits\n        # Accumulate the test loss.\n        total_eval_loss += loss.item()\n        # Move logits and labels to CPU\n        logits = logits.detach().cpu().numpy()\n        label_ids = b_labels.to('cpu').numpy()\n        # Store predictions and true labels\n        predictions.append(logits)\n        true_labels.append(label_ids)\n\n    # Combine the results across all batches.\n    flat_predictions = np.concatenate(predictions, axis=0)\n    flat_true_labels = np.concatenate(true_labels, axis=0)\n    # For each sample, pick the label (0, 1) with the highest score.\n    predicted_labels = np.argmax(flat_predictions, axis=1).flatten()\n    # Calculate the test accuracy.\n    val_accuracy = (predicted_labels == flat_true_labels).mean()\n\n    # Calculate the average loss over all of the batches.\n    avg_val_loss = total_eval_loss / len(test_dataloader)\n    \n    ov_acc = [accuracy_score(predicted_labels, flat_true_labels), recall_score(predicted_labels, flat_true_labels, average=\"macro\"), precision_score(predicted_labels, flat_true_labels, average=\"macro\"),f1_score(predicted_labels, flat_true_labels, average=\"macro\")]\n    f1 = list(f1_score(flat_true_labels,predicted_labels,average=None))\n    matrix = confusion_matrix(flat_true_labels,predicted_labels)\n    acc = list(matrix.diagonal()/matrix.sum(axis=1))\n    cr = pd.DataFrame(classification_report(pd.Series(flat_true_labels),pd.Series(predicted_labels), output_dict=True)).transpose().iloc[0:3, 0:2]\n    prec =list(cr.iloc[:,0])\n    rec = list(cr.iloc[:,1]) \n\n    # Report the final accuracy for this test run.\n    print(\"  0: {0:.3f}\".format(acc[0]))\n    print(\"  1: {0:.3f}\".format(acc[1]))\n    print(\"  2: {0:.3f}\".format(acc[2]))\n    print('BERT Prediction accuracy: {:.3f}'.format(val_accuracy))\n    \n    # Measure how long the test run took.\n    test_time = hf.format_time(time.time() - t0)\n    print(\"  Test Loss: {0:.3f}\".format(avg_val_loss))\n    print(\"  Test took: {:}\".format(test_time))        \n\n    fold_stats.append(\n        {\n            'fold': fold+1,\n            'Training Loss': avg_train_loss,\n            'Test Loss': avg_val_loss,\n            'Test Accur.': ov_acc[0],\n            '0 Accur.': acc[0],\n            '1 Accur.': acc[1],\n            '2 Accur.': acc[2],\n            'f1': [f1, ov_acc[3]],\n            'prec': [prec, ov_acc[2]],\n            'rec': [rec, ov_acc[1]]\n        }\n    )   "
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.mean([x['f1'][1] for x in fold_stats if x['f1'][0][0] > 0]))\n",
    "print(np.mean([x['f1'][0][0] for x in fold_stats if x['f1'][0][0] > 0]))\n",
    "print(np.mean([x['f1'][0][1] for x in fold_stats if x['f1'][0][0] > 0]))\n",
    "print(np.mean([x['f1'][0][2] for x in fold_stats if x['f1'][0][0] > 0]))\n",
    "print(np.mean([x['prec'] for x in fold_stats]))\n",
    "print(np.mean([x['rec'] for x in fold_stats]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[x['f1'][0][0] for x in fold_stats]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "fakenews_stats = []\n",
    "fakenews_stats.append(\n",
    "    {\n",
    "        'Model': model_name,\n",
    "        'lr': lr,\n",
    "        'epochs': epochs,\n",
    "        'batch_size': batch_size,\n",
    "        'tok': max_len,\n",
    "        \n",
    "        'fake_mean': np.mean([x['0 Accur.'] for x in fold_stats ]),\n",
    "        'fake_mean_sd': np.std([x['0 Accur.'] for x in fold_stats ]),\n",
    "        'fake_mean_f1': np.mean([x['f1'][0][0] for x in fold_stats ]),\n",
    "        'fake_mean_f1_sd': np.std([x['f1'][0][0] for x in fold_stats ]),\n",
    "        'fake_recall': np.mean([x['rec'][0][0] for x in fold_stats ]),\n",
    "        'fake_recall_sd': np.std([x['rec'][0][0] for x in fold_stats ]),\n",
    "        'fake_prec': np.mean([x['prec'][0][0] for x in fold_stats ]),\n",
    "        'fake_prec_sd': np.std([x['prec'][0][0] for x in fold_stats ]),\n",
    "        \n",
    "        'true_mean': np.mean([x['1 Accur.'] for x in fold_stats ]),\n",
    "        'true_mean_sd': np.std([x['1 Accur.'] for x in fold_stats ]),\n",
    "        'true_mean_f1': np.mean([x['f1'][0][1] for x in fold_stats ]),\n",
    "        'true_mean_f1_sd': np.std([x['f1'][0][1] for x in fold_stats ]),\n",
    "        'true_recall': np.mean([x['rec'][0][1] for x in fold_stats ]),\n",
    "        'true_recall_sd': np.std([x['rec'][0][1] for x in fold_stats ]),\n",
    "        'true_prec': np.mean([x['prec'][0][1] for x in fold_stats ]),\n",
    "        'true_prec_sd': np.std([x['prec'][0][1] for x in fold_stats ]),\n",
    "        \n",
    "        'undet_mean': np.mean([x['2 Accur.'] for x in fold_stats ]),\n",
    "        'undet_mean_sd': np.std([x['2 Accur.'] for x in fold_stats ]),\n",
    "        'undet_mean_f1': np.mean([x['f1'][0][2] for x in fold_stats ]),\n",
    "        'undet_mean_f1_sd': np.std([x['f1'][0][2] for x in fold_stats ]),\n",
    "        'undet_recall': np.mean([x['rec'][0][2] for x in fold_stats ]),\n",
    "        'undet_recall_sd': np.std([x['rec'][0][2] for x in fold_stats ]),\n",
    "        'undet_prec': np.mean([x['prec'][0][2] for x in fold_stats ]),\n",
    "        'undet_prec_sd': np.std([x['prec'][0][2] for x in fold_stats ]),        \n",
    "        \n",
    "        'overall_mean': np.mean([x['Test Accur.'] for x in fold_stats ]),\n",
    "        'overall_mean_sd': np.std([x['Test Accur.'] for x in fold_stats ]),\n",
    "        'overall_mean_f1': np.mean([x['f1'][1] for x in fold_stats ]),\n",
    "        'overall_mean_f1_sd': np.std([x['f1'][1] for x in fold_stats ]),\n",
    "        'overall_recall': np.mean([x['rec'][1] for x in fold_stats ]),\n",
    "        'overall_recall_sd': np.std([x['rec'][1] for x in fold_stats ]),\n",
    "        'overall_prec': np.mean([x['prec'][1] for x in fold_stats ]),\n",
    "        'overall_prec_sd': np.std([x['prec'][1] for x in fold_stats ]),\n",
    "    }\n",
    ")  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('fold_stats_bert_' + timestamp + '.txt', 'w') as outfile:\n",
    "  json.dump(fold_stats, outfile)\n",
    "with open('fakenews_results_bert_' + timestamp + '.txt', 'w') as outfile:\n",
    "  json.dump(fakenews_stats, outfile)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}